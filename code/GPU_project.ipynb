{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NO_C5o9-xRF_",
        "jRnZtx7ZbL-H",
        "JzT7dTvNZ4U1"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO_C5o9-xRF_"
      },
      "source": [
        "# ▶️ CUDA setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fekR2O4xRGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ada58cb-ad53-4ac0-d0c2-6c601a3a512d"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Psl9iouxRGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c5fd669-8d57-42c5-eb47-fead0246c54c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec  5 19:08:03 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU computing notebooks download (from github)"
      ],
      "metadata": {
        "id": "gcg1GyK5srek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/giulianogrossi/GPUcomputing.git"
      ],
      "metadata": {
        "id": "tyHOxci3s3H8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4e29469-da73-4225-c69b-12951ebab0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GPUcomputing'...\n",
            "remote: Enumerating objects: 575, done.\u001b[K\n",
            "remote: Counting objects: 100% (228/228), done.\u001b[K\n",
            "remote: Compressing objects: 100% (118/118), done.\u001b[K\n",
            "remote: Total 575 (delta 100), reused 214 (delta 98), pack-reused 347 (from 1)\u001b[K\n",
            "Receiving objects: 100% (575/575), 2.79 MiB | 16.31 MiB/s, done.\n",
            "Resolving deltas: 100% (257/257), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "227eLdP5csN1"
      },
      "source": [
        "NVCC Plugin for Jupyter notebook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd GPUcomputing/utils/nvcc4jupyter-master/\n",
        "!!python3 -m build\n",
        "%load_ext nvcc4jupyter\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "4TzxMBFds8aT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae7e413-852d-49e3-f8fe-c830df507cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPUcomputing/utils/nvcc4jupyter-master\n",
            "Source files will be saved in \"./src\".\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ▶️ Device Query"
      ],
      "metadata": {
        "id": "jRnZtx7ZbL-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DeviceQuery dell'attuale device (su Colab!)\n",
        "!nvcc -arch=sm_75 /content/GPUcomputing/utils/deviceQuery.cu -o deviceQuery\n",
        "!./deviceQuery"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGTU9Y8lbTJf",
        "outputId": "80a3e0f5-f5ed-4354-8a5a-111bfe60d1a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"Tesla T4\"\n",
            "  CUDA Driver Version / Runtime Version          12.2 / 12.2\n",
            "  GPU arch name:                                 Turing\n",
            "  CUDA Capability Major/Minor version number:    7.5\n",
            "  Total amount of global memory:                 15102 MBytes (15835660288 bytes)\n",
            "  (40) Multiprocessors, ( 64) CUDA Cores/MP:     2560 CUDA Cores\n",
            "  GPU Max Clock rate:                            1590 MHz (1.59 GHz)\n",
            "  Memory Clock rate:                             5001 Mhz\n",
            "  Memory Bus Width:                              256-bit\n",
            "  L2 Cache Size:                                 4194304 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n",
            "  Total amount of constant memory                65536 bytes\n",
            "  Total amount of shared memory per block        49152 bytes\n",
            "  Total number of registers available per block  65536\n",
            "  Warp size                                      32\n",
            "  Maximum number of threads per multiprocessor   1024\n",
            "  Maximum number of threads per block            1024\n",
            "  Max dimension size of a thread block (x,y,z)  (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z)  (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch                           2147483647 bytes\n",
            "  Texture alignment                              512 bytes\n",
            "  Concurrent copy and kernel execution           Yes with 3 copy engine(s)\n",
            "  Run time limit on kernels                      No\n",
            "  Integrated GPU sharing Host Memory             No\n",
            "  Support host page-locked memory mapping        Yes\n",
            "  Alignment requirement for Surfaces             Yes\n",
            "  Device has ECC support                         Enabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  ✅ Confronto algoritmi\n"
      ],
      "metadata": {
        "id": "NEumKSTrpxiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda_group_save --name main.cu --group \"progetto_GPU\"\n",
        "#include \"sequential_dijkstra.h\"\n",
        "#include \"sourcePartitioned.h\"\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <limits.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"/content/GPUcomputing/utils/common.h\"\n",
        "\n",
        "#define INF 999\n",
        "#define N 1024\n",
        "\n",
        "int main () {\n",
        "\n",
        "    double start, stopPartitoned, stopParallel, stopCPU, speedupPartitioned,speedupParallel;\n",
        "    /*alloco la memoria dinamicamente, la matrice è lunga NxN , rappresentata linearmente*/\n",
        "    int* matrice = (int*)malloc(N * N * sizeof(int));\n",
        "\n",
        "    /*Verifica l allocazione della memoria*/\n",
        "    if (matrice == NULL) {\n",
        "        printf(\"Errore di allocazione della memoria\\n\");\n",
        "        exit(1);\n",
        "    }\n",
        "\n",
        "     for (int i = 0; i < N; i++) {\n",
        "        matrice[i] = INF;\n",
        "     }\n",
        "\n",
        "    randomGraph(matrice, N);\n",
        "\n",
        "    printf(\"Random graph of %d nodes initialized\\n\", N);\n",
        "    /*printGraph(matrice, N)*/;\n",
        "\n",
        "    /*alloco matrice dei risultati su CPU per il partitioned*/\n",
        "    int* resultsPartitioned = (int*)malloc(N * N * sizeof(int));\n",
        "    int* resultsParallel = (int*)malloc(N * N * sizeof(int));\n",
        "\n",
        "    /*alloco matrice della GPU*/\n",
        "    int* gpu_matrix;\n",
        "    cudaError_t cudaError = cudaMalloc(&gpu_matrix, N * N * sizeof(int));\n",
        "\n",
        "    if (cudaError != cudaSuccess) {\n",
        "        printf(\"Errore during matrix allocation on GPU: %s\\n\", cudaGetErrorString(cudaError));\n",
        "        exit(1);\n",
        "    }\n",
        "    printf(\"Matrix allocation on GPU completed\\n\");\n",
        "\n",
        "    /*copio matrice da CPU a GPU*/\n",
        "    cudaError = cudaMemcpy(gpu_matrix, matrice, N * N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    if (cudaError != cudaSuccess) {\n",
        "        printf(\"Error during matrix copy on GPU: %s\\n\", cudaGetErrorString(cudaError));\n",
        "        exit(1);\n",
        "    }\n",
        "    printf(\"Matrix copy on GPU completed\\n\");\n",
        "\n",
        "\n",
        "    /*per memorizzare i risultati per il sequenziale*/\n",
        "    int* results = (int*)malloc(N * N * sizeof(int));\n",
        "\n",
        "    /*per memorizzare i risultati su GPU*/\n",
        "    int* resultsMatrix;\n",
        "    cudaError = cudaMalloc(&resultsMatrix, N * N * sizeof(int));\n",
        "\n",
        "    if (cudaError != cudaSuccess) {\n",
        "        printf(\"Error during results matrix allocation on GPU: %s\\n\", cudaGetErrorString(cudaError));\n",
        "        exit(1);\n",
        "    }\n",
        "    printf(\"Results matrix allocation completed\\n\");\n",
        "\n",
        "    /*array l su GPU*/\n",
        "    int* lArray;\n",
        "    cudaError = cudaMalloc(&lArray, N * N * sizeof(int));\n",
        "\n",
        "    if (cudaError != cudaSuccess) {\n",
        "        printf(\"Error during results lArray allocation on GPU: %s\\n\", cudaGetErrorString(cudaError));\n",
        "        exit(1);\n",
        "    }\n",
        "    printf(\"lArray allocation completed\\n\");\n",
        "\n",
        "    /*nodi visitati su GPU*/\n",
        "    bool* VtArray;\n",
        "    cudaError = cudaMalloc(&VtArray, N * N * sizeof(bool));\n",
        "\n",
        "    if (cudaError != cudaSuccess) {\n",
        "        printf(\"Error during results VtArray allocation on GPU: %s\\n\", cudaGetErrorString(cudaError));\n",
        "        exit(1);\n",
        "    }\n",
        "    printf(\"VtArray allocation completed\\n\");\n",
        "\n",
        "\n",
        "    printf(\"Matrice di adiacenza del grafo casuale:\\n\");\n",
        "\n",
        "    for (int i = 0; i < N * N; i++) {\n",
        "        results[i] = 0;\n",
        "    }\n",
        "\n",
        "    start = seconds();\n",
        "    dijkstraCPU((int*)matrice, N,(int*)results);\n",
        "    stopCPU = seconds() - start;\n",
        "    printf(\"Tempo di esecuzione CPU: %f\\n\",stopCPU);\n",
        "    free(matrice);\n",
        "\n",
        "    printf(\"\\n\\nSOURCE PARTITONED PART\\n\\n\");\n",
        "\n",
        "    int threadsPerBlock = 1024;\n",
        "    int blocks = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    start = seconds();\n",
        "    sourcePartitioned<<< blocks, threadsPerBlock >>> (gpu_matrix, N, resultsMatrix, lArray, VtArray);\n",
        "\n",
        "    cudaError = cudaGetLastError();\n",
        "\n",
        "    if (cudaError != cudaSuccess) {\n",
        "        printf(\"Error during kernel SOURCE PARTITONED launch: %s\\n\", cudaGetErrorString(cudaError));\n",
        "        exit(1);\n",
        "    }\n",
        "\n",
        "    cudaError = cudaDeviceSynchronize();\n",
        "    if (cudaError != cudaSuccess) {\n",
        "        printf(\"Kernel syncronization returned error: %s\\n\", cudaGetErrorString(cudaError));\n",
        "        exit(1);\n",
        "    }\n",
        "    stopPartitoned = seconds() - start;\n",
        "    printf(\"Tempo di esecuzione Source Partitioned: %f\\n\",stopPartitoned);\n",
        "\n",
        "    cudaError = cudaMemcpy(results, resultsMatrix, N * N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    if (cudaError != cudaSuccess) {\n",
        "        printf(\"Error during results copy on Host: %s\\n\", cudaGetErrorString(cudaError));\n",
        "    }\n",
        "    printf(\"Results copy on Host completed\\n\");\n",
        "\n",
        "    for (int i = 0; i < N * N; i++) {\n",
        "        resultsPartitioned[i] = results[i];\n",
        "    }\n",
        "\n",
        "    cudaFree(lArray);\n",
        "    cudaFree(VtArray);\n",
        "\n",
        "\n",
        "\n",
        "    if (N <= 1024) {\n",
        "        printf(\"\\n\\nSOURCE PARALLEL\\n\\n\");\n",
        "\n",
        "        // Inizializzo di nuovo la matrice dei risultati\n",
        "        for (int i = 0; i < N * N; i++) {\n",
        "            results[i] = 0;\n",
        "        }\n",
        "\n",
        "        size_t sharedMemSize = sizeof(int) * N * 3 + sizeof(bool) * (N + 1);\n",
        "\n",
        "        start=seconds();\n",
        "        sourceParallel << <N, N, sharedMemSize >> > (gpu_matrix, resultsMatrix);\n",
        "\n",
        "        cudaError = cudaGetLastError();\n",
        "\n",
        "        if (cudaError != cudaSuccess) {\n",
        "            printf(\"Error during kernel SOURCE PARALLEL launch: %s\\n\", cudaGetErrorString(cudaError));\n",
        "            exit(1);\n",
        "        }\n",
        "\n",
        "        cudaError = cudaPeekAtLastError();\n",
        "        if (cudaError != cudaSuccess) {\n",
        "            printf(\"Error during kernel SOURCE PARALLEL execution: %s\\n\", cudaGetErrorString(cudaError));\n",
        "            exit(1);\n",
        "        }\n",
        "\n",
        "        cudaError = cudaDeviceSynchronize();\n",
        "        if (cudaError != cudaSuccess) {\n",
        "            printf(\"Kernel SOURCE PARALLEL syncronization returned error: %s\\n\", cudaGetErrorString(cudaError));\n",
        "            exit(1);\n",
        "        }\n",
        "\n",
        "        stopParallel = seconds() - start;\n",
        "        printf(\"Tempo di esecuzione Source Parallel: %f\\n\",stopParallel);\n",
        "\n",
        "        cudaError = cudaMemcpy(results, resultsMatrix, N * N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "        if (cudaError != cudaSuccess) {\n",
        "            printf(\"Error during results copy on Host: %s\\n\", cudaGetErrorString(cudaError));\n",
        "        }\n",
        "        printf(\"Results copy on Host completed\\n\");\n",
        "\n",
        "        for (int i = 0; i < N * N; i++) {\n",
        "            resultsParallel[i] = results[i];\n",
        "        }\n",
        "\n",
        "    }\n",
        "\n",
        "    speedupPartitioned = stopCPU / stopPartitoned;\n",
        "    printf(\"Speedup Partitoned: %f\\n\", speedupPartitioned);\n",
        "\n",
        "    speedupParallel = stopCPU / stopParallel;\n",
        "    printf(\"Speedup Parallel: %f\\n\", speedupParallel);\n",
        "\n",
        "    cudaFree(resultsMatrix);\n",
        "    cudaFree(gpu_matrix);\n",
        "\n",
        "    free(results);\n",
        "    free(matrice);\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "tS-jDqfNqmBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Algoritmo sequenziale Dijkstra\n"
      ],
      "metadata": {
        "id": "JzT7dTvNZ4U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda_group_save --name sequential_dijkstra.cu --group \"progetto_GPU\"\n",
        "#include \"sequential_dijkstra.h\"\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <limits.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define INF 9999\n",
        "\n",
        "\n",
        "\n",
        "bool tuttiVeri(bool* vector, int dimension) {\n",
        "    for (int i = 0; i < dimension; i++) {\n",
        "        if (vector[i] == false) return false;\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "\n",
        "int getRandomWeight() {\n",
        "    /*pesi possibili grafo*/\n",
        "    int list[] = { 1, 2, 3, 4, 5, INF };\n",
        "    int index = rand() % (sizeof(list) / sizeof(list[0]));\n",
        "    return list[index];\n",
        "}\n",
        "\n",
        "\n",
        "void dijkstraCPU(int* matrice, int n, int* results) {\n",
        "        printf(\"CPU\\n\");\n",
        "\n",
        "    for (int nodo = 0; nodo < n; nodo++) {\n",
        "      int distanza[n];\n",
        "      bool V_t[n];\n",
        "      for (int i = 0; i < n; i++) {\n",
        "          V_t[i] = false;\n",
        "      }\n",
        "      V_t[nodo] = true;\n",
        "\n",
        "\n",
        "      for (int i = 0; i < n; i++) {\n",
        "          distanza[i] = matrice[nodo * n + i];\n",
        "      }\n",
        "\n",
        "    while (tuttiVeri(V_t, n) == false) {\n",
        "            int minDist = INF;\n",
        "            int minIdx = nodo;\n",
        "\n",
        "            /*trovo il nodo non visitato con la distanza minima dalla sorgente*/\n",
        "            for (int i = 0; i < n; i++) {\n",
        "                if (V_t[i] == false && distanza[i] < minDist) {\n",
        "                    minDist = distanza[i];\n",
        "                    minIdx = i;\n",
        "                }\n",
        "            }\n",
        "           V_t[minIdx] = true;\n",
        "\n",
        "           /*ricalcolo distanze per vertici non in V_t*/\n",
        "           for (int j = 0; j < n; j++) {\n",
        "              if (V_t[j] == false && matrice[minIdx * n + j] != 0 && distanza[minIdx] != INF && distanza[minIdx] + matrice[minIdx * n + j] < distanza[j])\n",
        "                  distanza[j] = distanza[minIdx] + matrice[minIdx * n + j];\n",
        "                    }\n",
        "    }\n",
        "    /*\n",
        "      printf(\"Distanze dal nodo %d:\\n\", nodo);\n",
        "    for (int i = 0; i < n; i++) {\n",
        "      if (distanza[i] == INF)\n",
        "          printf(\"Nodo %d: INFINITO\\n\", i);\n",
        "      else\n",
        "          printf(\"Nodo %d: %d\\n\", i, distanza[i]);\n",
        "      }\n",
        "    */\n",
        "\n",
        "     for (int i = 0; i < n; i++) {\n",
        "            results[nodo * n + i] = distanza[i];\n",
        "        }\n",
        "\n",
        "    }\n",
        "\n",
        "  }\n",
        "\n",
        "\n",
        "\n",
        "void randomGraph(int* matrix, int dimension) {\n",
        "    srand(time(NULL));\n",
        "    /* i è la riga,j la colonna*/\n",
        "    for (int i = 0; i < dimension; i++) {\n",
        "        for (int j = 0; j < dimension; j++) {\n",
        "            if (i == j) {\n",
        "                matrix[i * dimension + j] = 0;\n",
        "            } else {\n",
        "                int weight = getRandomWeight();\n",
        "                matrix[i * dimension + j] = weight;\n",
        "                matrix[j * dimension + i] = weight;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "void printGraph(int* matrix, int dimension) {\n",
        "    for (int i = 0; i < dimension; i++) {\n",
        "        for (int j = 0; j < dimension; j++) {\n",
        "            if (matrix[i * dimension + j] == INF) {\n",
        "                printf(\"INF \");\n",
        "            } else {\n",
        "                printf(\"%d \", matrix[i * dimension + j]);\n",
        "            }\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "mdBo9V6AZ_2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda_group_save --name sequential_dijkstra.h --group \"progetto_GPU\"\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "extern bool tuttiVeri(bool* vector, int dimension);\n",
        "\n",
        "extern void dijkstraCPU(int* matrice, int n, int* results);\n",
        "extern void randomGraph(int* matrix, int dimension);\n",
        "extern void printGraph(int* matrix, int dimension);"
      ],
      "metadata": {
        "id": "0awTc4Bnmivs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 src/progetto_GPU/sequential_dijkstra.cu -o sequential_dijkstra\n",
        "!./sequential_dijkstra"
      ],
      "metadata": {
        "id": "sWXyzqHLjdUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "450c20aa-036e-4af3-ee48-aca512d2c4e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/ld: /usr/lib/gcc/x86_64-linux-gnu/11/../../../x86_64-linux-gnu/Scrt1.o: in function `_start':\n",
            "(.text+0x1b): undefined reference to `main'\n",
            "collect2: error: ld returned 1 exit status\n",
            "/bin/bash: line 1: ./sequential_dijkstra: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ✅ Algoritmo Source Partitioned e Source Parallel Dijkstra"
      ],
      "metadata": {
        "id": "NKRRjo-yQgkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda_group_save --name sourcePartitioned.cu --group \"progetto_GPU\"\n",
        "#include \"sourcePartitioned.h\"\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <limits.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define INF 999\n",
        "#define INF 999\n",
        "\n",
        "#ifdef __CUDACC__\n",
        "#define __syncthreads() __syncthreads()\n",
        "#else\n",
        "#define __syncthreads()\n",
        "#endif\n",
        "\n",
        "__device__ int minimoIndiceVt(int a, int b, int nodeIndexA, int nodeIndexB, bool VtA, bool VtB) {\n",
        "    if (VtA && VtB == false) return nodeIndexB;\n",
        "    if (VtB && VtA == false) return nodeIndexA;\n",
        "    int minimum = min(a, b);\n",
        "    return minimum == a ? nodeIndexA : nodeIndexB;\n",
        "}\n",
        "\n",
        "__device__ int minimoVt(int a, int b, bool VtA, bool VtB) {\n",
        "    if (VtA && VtB == false) return b;\n",
        "    if (VtB && VtA == false) return a;\n",
        "    return min(a, b);\n",
        "}\n",
        "\n",
        "__device__ bool tuttiVeriGPU(bool* vector, int dimension) {\n",
        "    for (int i = 0; i < dimension; i++) {\n",
        "        if (vector[i] == false) return false;\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "__global__ void sourcePartitioned(int* matrice, int dimension, int* results, int* lArray, bool* VtArray) {\n",
        "\n",
        "\n",
        "    /*Trovo l indice globale del thread*/\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < dimension) {\n",
        "\n",
        "        /*prendo solo la parte di VtArray riguardante il mio nodo, punto l indirizzo che ha idx*dimension posizioni dopo l inizio di VtArray*/\n",
        "        bool* V_t = VtArray + idx * dimension;\n",
        "        /*inizializzo V_t*/\n",
        "        for (int i = 0; i < dimension; i++) {\n",
        "                V_t[i] = false;\n",
        "            }\n",
        "        V_t[idx] = true;\n",
        "\n",
        "        /*prendo la parte di lArray riguardante il nodo corrente*/\n",
        "        int* distanze = lArray + idx * dimension;\n",
        "        /*inizializzo il vettore delle distanze*/\n",
        "        for (int i = 0; i < dimension; i++) {\n",
        "            distanze[i] = matrice[idx * dimension + i];\n",
        "        }\n",
        "\n",
        "        while (tuttiVeriGPU(V_t, dimension) == false) {\n",
        "\n",
        "            int minDist = INF;\n",
        "            int minIdx = idx;\n",
        "\n",
        "            /*trovo il nodo non visitato con la distanza minima dalla sorgente*/\n",
        "            for (int i = 0; i < dimension; i++) {\n",
        "                if (V_t[i] == false && distanze[i] < minDist) {\n",
        "                    minDist = distanze[i];\n",
        "                    minIdx = i;\n",
        "                }\n",
        "            }\n",
        "\n",
        "            /* aggiungo a V_t il nodo piu vicino*/\n",
        "            V_t[minIdx] = true;\n",
        "\n",
        "            /*ricalcolo distanze per vertici non in V_t*/\n",
        "           for (int j = 0; j < dimension; j++) {\n",
        "              if (V_t[j] == false && matrice[minIdx * dimension + j] != 0 && distanze[minIdx] != INF && distanze[minIdx] + matrice[minIdx * dimension + j] < distanze[j])\n",
        "                  distanze[j] = distanze[minIdx] + matrice[minIdx * dimension + j];\n",
        "                    }\n",
        "        }\n",
        "\n",
        "        /*\n",
        "         if(idx==0){\n",
        "          printf(\"GPU\\n\");\n",
        "\n",
        "          printf(\"Distanze dal nodo %d:\\n\", idx);\n",
        "\n",
        "          for (int i = 0; i < dimension; i++) {\n",
        "            if (distanze[i] == INF)\n",
        "              printf(\"Nodo %d: INFINITO\\n\", i);\n",
        "            else\n",
        "              printf(\"Nodo %d: %d\\n\", i, distanze[i]);\n",
        "            }\n",
        "\n",
        "         }*/\n",
        "\n",
        "\n",
        "        for (int i = 0; i < dimension; i++) {\n",
        "            results[idx * dimension + i] = distanze[i];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void sourceParallel(int* matrix, int* results) {\n",
        "    int tID = threadIdx.x;\n",
        "    int bID = blockIdx.x;\n",
        "    int bDim = blockDim.x;\n",
        "\n",
        "    extern __shared__ int s[];\n",
        "    int* sharedData = s;\n",
        "\n",
        "    int* distanze = (int*)&sharedData[0];\n",
        "\n",
        "\n",
        "    int* valoriMinimi = (int*)&distanze[bDim];\n",
        "    int* indiciMinimi = (int*)&valoriMinimi[bDim];\n",
        "\n",
        "    bool* V_t = (bool*)&indiciMinimi[bDim];\n",
        "\n",
        "    bool* conclusione = (bool*)&V_t[bDim];\n",
        "\n",
        "    V_t[tID] = false;\n",
        "\n",
        "    if (tID == 0) {\n",
        "        V_t[bID] = true;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    distanze[tID] = matrix[bID * bDim + tID];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    while (true) {\n",
        "\n",
        "        if (tID == 0) {\n",
        "            conclusione[0] = tuttiVeriGPU(V_t, bDim);\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        if (conclusione[0]) {\n",
        "            break;\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        valoriMinimi[tID] = distanze[tID];\n",
        "        indiciMinimi[tID] = tID;\n",
        "\n",
        "        for (int stride = 1; stride < bDim; stride *= 2) {\n",
        "            int index = 2 * stride * tID;\n",
        "\n",
        "            if (index + stride < bDim) {\n",
        "\n",
        "                int minimoLocale = minimoVt(\n",
        "                    valoriMinimi[index],\n",
        "                    valoriMinimi[index + stride],\n",
        "                    V_t[indiciMinimi[index]],\n",
        "                    V_t[indiciMinimi[index + stride]]\n",
        "                );\n",
        "\n",
        "                int indiceMinimoLocale = minimoIndiceVt(\n",
        "                    valoriMinimi[index],\n",
        "                    valoriMinimi[index + stride],\n",
        "                    indiciMinimi[index],\n",
        "                    indiciMinimi[index + stride],\n",
        "                    V_t[indiciMinimi[index]],\n",
        "                    V_t[indiciMinimi[index + stride]]\n",
        "                );\n",
        "\n",
        "                indiciMinimi[index] = indiceMinimoLocale;\n",
        "                valoriMinimi[index] = minimoLocale;\n",
        "\n",
        "            }\n",
        "            __syncthreads();\n",
        "        }\n",
        "\n",
        "        if (tID == 0) {\n",
        "            V_t[indiciMinimi[0]] = true;\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        if (V_t[tID]== false) {\n",
        "\n",
        "            int pesoUv = matrix[indiciMinimi[0] * bDim + tID];\n",
        "            distanze[tID] = min(distanze[tID], distanze[indiciMinimi[0]] + pesoUv);\n",
        "\n",
        "        }\n",
        "\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "    }\n",
        "\n",
        "/*\n",
        "if (tID == 0) {\n",
        "    printf(\"Iterazione iniziale, distanze dal nodo %d:\\n\", bID);  // bID rappresenta il nodo di partenza\n",
        "    for (int i = 0; i < bDim; i++) {\n",
        "        if (distanze[i] == INF) {\n",
        "            printf(\"Nodo %d -> Nodo %d: INFINITO\\n\", bID, i);  // Nodo di partenza bID verso nodo i\n",
        "        } else {\n",
        "            printf(\"Nodo %d -> Nodo %d: %d\\n\", bID, i, distanze[i]);  // Nodo di partenza bID verso nodo i con distanza\n",
        "        }\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "}\n",
        "*/\n",
        "\n",
        "    results[bID * bDim + tID] = distanze[tID];\n",
        "}\n"
      ],
      "metadata": {
        "id": "zYiP1q9RQrsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda_group_save --name sourcePartitioned.h --group \"progetto_GPU\"\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "__device__ extern bool tuttiVeriGPU(bool* vector, int dimension);\n",
        "\n",
        "__device__ int minimoIndiceVt(int a, int b, int nodeIndexA, int nodeIndexB, bool VtA, bool VtB);\n",
        "\n",
        "__device__ int minimoVt(int a, int b, bool VtA, bool VtB);\n",
        "\n",
        "__global__ extern void sourcePartitioned(int* matrice, int dimension, int* results, int* lArray, bool* VtArray);\n",
        "\n",
        "__global__ void sourceParallel(int* matrix, int* results);\n",
        "\n"
      ],
      "metadata": {
        "id": "n8mf7FdJZlL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 src/progetto_GPU/sourcePartitioned.cu main.cu -o sourcePartitioned\n",
        "!./sourcePartitioned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJ2qN80p-4YX",
        "outputId": "426a2cb3-fea1-42aa-d52c-1959f6b2139a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kmain.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./sourcePartitioned: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Esecuzione"
      ],
      "metadata": {
        "id": "-hWDLHAiGQJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -o main src/progetto_GPU/main.cu src/progetto_GPU/sequential_dijkstra.cu src/progetto_GPU/sourcePartitioned.cu\n",
        "!./main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c73f73f0-1885-40d9-8853-b3d196391488",
        "id": "K2Wc9CN0GUMZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random graph of 1024 nodes initialized\n",
            "Matrix allocation on GPU completed\n",
            "Matrix copy on GPU completed\n",
            "Results matrix allocation completed\n",
            "lArray allocation completed\n",
            "VtArray allocation completed\n",
            "Matrice di adiacenza del grafo casuale:\n",
            "CPU\n",
            "Tempo di esecuzione CPU: 9.172480\n",
            "\n",
            "\n",
            "SOURCE PARTITONED PART\n",
            "\n",
            "Tempo di esecuzione Source Partitioned: 3.614423\n",
            "Results copy on Host completed\n",
            "\n",
            "\n",
            "SOURCE PARALLEL\n",
            "\n",
            "Tempo di esecuzione Source Parallel: 0.610411\n",
            "Results copy on Host completed\n",
            "Speedup Partitoned: 2.537744\n",
            "Speedup Parallel: 15.026724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu ./main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTOPMQoZ-5Xz",
        "outputId": "e9465e30-1846-4931-d422-cc285f4a1a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random graph of 2048 nodes initialized\n",
            "==PROF== Connected to process 16454 (/content/main)\n",
            "Matrix allocation on GPU completed\n",
            "Matrix copy on GPU completed\n",
            "Results matrix allocation completed\n",
            "lArray allocation completed\n",
            "VtArray allocation completed\n",
            "Matrice di adiacenza del grafo casuale:\n",
            "CPU\n",
            "==PROF== Received signal\n",
            "==PROF== Trying to shutdown target application\n",
            "==WARNING== No kernels were profiled.\n",
            "==WARNING== Profiling kernels launched by child processes requires the --target-processes all option.\n"
          ]
        }
      ]
    }
  ]
}
